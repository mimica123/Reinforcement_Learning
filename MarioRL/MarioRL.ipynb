{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "4191dc80e4ba8664c294b99d58bfba03448f89d801b10a504e1674e74931ddca"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('practicalRL': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MarioRL.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTEm15uGcFW5",
        "outputId": "5f6af94c-8661-4b1b-8596-3af378cc8a36"
      },
      "source": [
        "!pip install gym-super-mario-bros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.1.2\n",
            "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.62.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym-super-mario-bros) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=435665 sha256=c3873bb29ede3ee18788385a187627e4448ffd08d81c06fed05f8e6c30ae2af3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.2 nes-py-8.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84ncZ46cFWz"
      },
      "source": [
        "#Importing Dependencies for Mario\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from keras.models import save_model\n",
        "from keras.models import load_model\n",
        "\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6dX7gtcFW6"
      },
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "env = JoypadSpace(env, RIGHT_ONLY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mEPwNa9cFW7"
      },
      "source": [
        "total_reward = 0\n",
        "done = True\n",
        "\n",
        "for step in range(100000):\n",
        "    #env.render() #comment this on colab\n",
        "\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    state, reward, done, info = env.step(env.action_space.sample())\n",
        "    #preprocess_state(state)\n",
        "    #print(state)\n",
        "    break\n",
        "    print(info)\n",
        "    total_reward += reward\n",
        "    clear_output(wait=True)\n",
        "\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVc5Q2y3cFW7"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        #Create vvariables for our agent\n",
        "        self.state_space = state_size\n",
        "        self.action_space = action_size\n",
        "        self.memory = deque(maxlen=5000)\n",
        "        self.gamma = 0.8\n",
        "        self.chosenAction = 0\n",
        "\n",
        "        #Exploration vs explotation\n",
        "        self.epsilon = 1\n",
        "        self.max_epsilon = 1\n",
        "        self.min_epsilon = 0.1\n",
        "        self.decay_epsilon = 0.0001\n",
        "\n",
        "        #Building Neural Networks for Agent\n",
        "        self.main_network = self.build_network()\n",
        "        self.target_network = self.build_network()\n",
        "        self.update_target_network()\n",
        "\n",
        "    def build_network(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(64, (4,4), strides=4, padding='same', input_shape=self.state_space))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Conv2D(64, (4,4), strides=2, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Conv2D(64, (3,3), strides=2, padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        model.add (Dense(512, activation='relu'))\n",
        "        model.add (Dense(256, activation='relu'))\n",
        "        model.add(Dense(self.action_space, activation='linear'))\n",
        "\n",
        "        model.compile(loss='mse', optimizer=Adam())\n",
        "\n",
        "        return model\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.set_weights(self.main_network.get_weights())\n",
        "\n",
        "    def act(self, state, onGround):\n",
        "        if onGround < 83:\n",
        "            print(\"On Ground\")\n",
        "            if random.uniform(0,1) < self.epsilon:\n",
        "                self.chosenAction = np.random.randit(self.action_space)\n",
        "                return self.chosenAction\n",
        "                #return np.random.randit(self, action_space)\n",
        "            Q_value = self.main_network.predict(state)\n",
        "            self.chosenAction = np.argmax(Q_value[0])\n",
        "            #print(Q_value)\n",
        "            return self.chosenAction\n",
        "        else:\n",
        "            print(\"Not on Ground\")\n",
        "            return self.chosenAction\n",
        "\n",
        "    def update_epsilon(self, episode):\n",
        "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * np.exp(-self.decay_epsilon * episode)\n",
        "    \n",
        "    #Train the Network\n",
        "    def train(self, batch_size):\n",
        "        #minibatch from memory\n",
        "        minibatch = random.sample(self.meemory, batch_size)\n",
        "\n",
        "        #Get variables form batch so we can find q-value\n",
        "        for state, action, reward,next_state, done in minibatch:\n",
        "            target = self.main_network.predict(state)\n",
        "\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else: \n",
        "                target[0][action] = (reward + self.gamma * np.amax(self.target_network.predict(next_state)))\n",
        "\n",
        "            self.main_network.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_pred_act(self, state):\n",
        "        Q_values = self.main_network.predict(state)\n",
        "        #print(Q_values)\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "    def load(self, name):\n",
        "        self.main_network = load_model(name)\n",
        "        self.target_network = load_model(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        save_model(self.main_network, name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bcsc0UacFW9"
      },
      "source": [
        "action_space = env.action_space.n\n",
        "state_space = (80, 88, 1)\n",
        "\n",
        "#env.observation_space\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def preprocess_state(state):\n",
        "    image = Image.fromarray(state)\n",
        "    image = image.resize((88, 80))\n",
        "    image = image.convert('L')\n",
        "    image = np.array(image)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ols7oOPcFW-"
      },
      "source": [
        "num_episodes = 1000000\n",
        "num_timesteps = 400000\n",
        "batch_size = 64\n",
        "DEBUG_LENGTH = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqFKKhzMcFW-"
      },
      "source": [
        "dqn = DQNAgent(state_space, action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpG_Y8fqcFW_"
      },
      "source": [
        "print('STARTING TRAINING')\n",
        "\n",
        "stuck_buffer = deque(maxlen=DEBUG_LENGTH)\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    Return = 0\n",
        "    done = False\n",
        "    time_step = 0\n",
        "    onGround = 79\n",
        "\n",
        "    state = preprocess_state(env.reset())\n",
        "    state = state.reshape(-1, 80, 88, 1)\n",
        "\n",
        "    for t in range(time_steps):\n",
        "    #env.render() #comment this on colab\n",
        "      time_steps += 1\n",
        "      if t > 1 and stuck_buffer.count(stuck_buffer[-1]) > DEBUG_LENGTH - 50:\n",
        "          action = dqn.act(state, onGround=79)\n",
        "      else:\n",
        "          action = dqn.act(state, onGround)\n",
        "    \n",
        "\n",
        "    print(\"ACTION IS \" + str(action))\n",
        "\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    #print(info['y_pos'])\n",
        "    onGround = info['y_pos']\n",
        "    stuck_buffer.append(info['x_pos'])\n",
        "    next_state = preprocess_state(next_state)\n",
        "    next_state = next_state.reshape(-1, 80, 88, 1)\n",
        "\n",
        "    dqn.store_transition(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "\n",
        "    Return += reward\n",
        "    print(\"Episode is: {}\\nTotal Time Step: {}\\nCurrent Reward: {}\\nEpsilon is: {}\".format(str(i), str(time_step), str(Return), str(dqn.epsilon)))\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "    if len(dqn.memory) > batch_size and i >5:\n",
        "        dqn.train(batch_size)\n",
        "\n",
        "dqn.update_epsilon(i)\n",
        "clear_output(wait=True)\n",
        "dqn.update_target_network()\n",
        "#Save Model\n",
        "\n",
        "env.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx2oQ0LdcFXA"
      },
      "source": [
        "dqn.save('marioRL.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o9yFroDcFXB"
      },
      "source": [
        "dqn.load('marioRL.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e_wOFPncFXB"
      },
      "source": [
        "#Visualizing Model\n",
        "\n",
        "while 1:\n",
        "    done = False\n",
        "    state = preprocess_state(env.reset())\n",
        "    state = state.reshape(-1, 80, 88, 1)\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = dqn.get_pred_act(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        next_state = preprocess_state(next_state)\n",
        "        state = state.reshape(-1, 80, 88, 1)\n",
        "        state = next_state\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XcH1Kahf_Of"
      },
      "source": [
        "env = gym.make('stock-v0', df =df, frame_bound=(200,253), window_size=5)\n",
        "obs = env.reset()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}